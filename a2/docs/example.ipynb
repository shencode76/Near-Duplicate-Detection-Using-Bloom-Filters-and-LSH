{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547bfbd1",
   "metadata": {},
   "source": [
    "# Example Illustration\n",
    "\n",
    "To use `a2` in a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35897a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "import a2\n",
    "\n",
    "print(a2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ce0be-c668-4721-884e-c870f0a08199",
   "metadata": {},
   "source": [
    "## Exercise1 from [page 59 of Algorithms and Data Structures for Massive Datasets](https://ebookcentral-proquest-com.proxy.library.georgetown.edu/lib/georgetown/reader.action?docID=7049417&ppg=64).\n",
    "\n",
    "Key Requirements:\n",
    "Bloom Filter Setup:\n",
    "- n = 10 ^ 7\n",
    "- f = 0.02 (target false positive rate = 2%)\n",
    "- Elements should be uniform random integers in the range [0, 10^12], converted into strings.\n",
    "\n",
    "Tasks:\n",
    "- Save the inserted elements into a file.\n",
    "- Perform 10^6 (1 million) lookups using randomly selected elements from the universe U in the range [0, 10^12].\n",
    "- Perform 10^6 (1 million) successful lookups using elements from the inserted elements.\n",
    "- Track the false positive rate and verify it is close to 2%.\n",
    "\n",
    "Time Measurement:\n",
    "- Measure the time required for the lookups, but exclude the time for random number generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ac8ac3-0382-4db1-9298-904073f4f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import mmh3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bitarray import bitarray\n",
    "# exercise 1\n",
    "import random\n",
    "import time\n",
    "from a2.bloomfilter1 import BloomFilter\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5eadd-c4fa-4310-9297-d390c4924ca3",
   "metadata": {},
   "source": [
    "### Initialize bloom filter with n and f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707a22a2-dc1d-47aa-8c50-93f0809249be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init parameters:\n",
      "n = 10000000, f=0.02, m=81423633, k=5\n"
     ]
    }
   ],
   "source": [
    "n = 10**7\n",
    "f = 0.02\n",
    "bf = BloomFilter(n, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf31eb5-5d83-4540-b6ed-2dca96864ad4",
   "metadata": {},
   "source": [
    "Randomly generate integer, convert to string, save to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c51ff7a-7aeb-4f01-aa65-b7ff9008081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/a2/inserted_elements.txt', 'w') as f:\n",
    "    for n in range(n): \n",
    "        random_integer = random.randint(0, 10**12)\n",
    "        random_string = str(random_integer)\n",
    "        bf.insert(random_string)\n",
    "        # Write the inserted element to the file\n",
    "        f.write(f\"{random_string}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf155338-b362-45b4-9720-97722c047710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100963958600', '254970694481', '259722675981', '438222137969', '700849256904']\n"
     ]
    }
   ],
   "source": [
    "lo_elements = []\n",
    "with open('../src/a2/inserted_elements.txt', 'r') as f:\n",
    "    lo_elements = [line.strip() for line in f]\n",
    "\n",
    "print(lo_elements[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e723da0-155f-4235-81dc-36ada2964cd0",
   "metadata": {},
   "source": [
    "### Uniform Random Lookup:\n",
    "Elements used: randomly generate elements from U, elements are not necessarily in the Bloom filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c59fad8-3da7-4bc3-bd6f-d0662d664805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required time for performing 10^6 lookups of uniformly randomly selected elements from U: 0.327 seconds\n",
      "False Positive rate during look-up is: 2.0303999999999998%\n"
     ]
    }
   ],
   "source": [
    "looks = 10 ** 6\n",
    "real_uniform_f = 0 # false positive rate in simulation\n",
    "# generate integer, convert to string\n",
    "lookup_elements = [str(random.randint(0, 10**12)) for l in range(looks)]\n",
    "start_time = time.time()\n",
    "for j in range(looks):\n",
    "    current_element = lookup_elements[j]\n",
    "    if bf.query(current_element):\n",
    "        real_uniform_f += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Required time for performing 10^6 lookups of uniformly randomly selected elements from U: {} seconds\".format(round(end_time-start_time, 3)))\n",
    "print(\"False Positive rate during look-up is: {}%\".format(real_uniform_f / looks * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7fbae-9a41-47eb-9fe6-289db20e61e6",
   "metadata": {},
   "source": [
    "\n",
    "### Successful lookups \n",
    "Perform lookups using elements that were actually inserted into the Bloom filter. Selecting these elements from the file of inserted elements (inserted_elements.txt), ensuring that each lookup will be successful \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f91756-cb4c-4cc4-9112-6cec8c028cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required time for performing 10^6 successful lookups: 1.115 seconds\n",
      "Rate of successfylly find element in Bloom Filter during successful look-up is: 100.0%\n"
     ]
    }
   ],
   "source": [
    "real_successful_r = 0 # false positive rate in successful lookups\n",
    "# generate integer, convert to string\n",
    "start_time2 = time.time()\n",
    "for z in range(looks):\n",
    "    current_element = random.choice(lo_elements)\n",
    "    if bf.query(current_element):\n",
    "        real_successful_r += 1\n",
    "\n",
    "end_time2 = time.time()\n",
    "print(\"Required time for performing 10^6 successful lookups: {} seconds\".format(round(end_time2-start_time2, 3)))\n",
    "print(\"Rate of successfylly find element in Bloom Filter during successful look-up is: {}%\".format(real_successful_r / looks * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138dd848",
   "metadata": {},
   "source": [
    "## EXERCISE 2 from [page 59 of Algorithms and Data Structures for Massive Datasets](https://ebookcentral-proquest-com.proxy.library.georgetown.edu/lib/georgetown/reader.action?docID=7049417&ppg=64).\n",
    "Using the provided implementation, create a Bloom filter such as the one in Example 2. Now create two other filters, one in which the dataset is 100 times larger than the original one, and another one in which the dataset is 100 times smaller, leaving the same false positive rate. What do you notice about the size of the filter as the dataset size changes?\n",
    "\n",
    "Key requirements from Example 2:\n",
    "- n = 10 ^ 6 elements\n",
    "- 1 MB available for storing (m = 8 * 10^6 bits)\n",
    "\n",
    "Tasks: \n",
    "- Construct a Bloom Filter whose dataset is 100 times larger than the original one(the one we created for example2)\n",
    "- Construct a Bloom Filter whose dataset is 100 times smaller than the original one(the one we created for example2)\n",
    "- Share findings of the size of the filter as the dataset size changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6abb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateF(k):\n",
    "    \"\"\"calculate false positive rate based on given k\n",
    "    return float: the value of f\n",
    "    \"\"\"\n",
    "    return (0.5) ** k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3a5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init parameters:\n",
      "n = 1000000, f=0, m=8000000, k=5\n",
      "Init parameters:\n",
      "n = 1000000, f=0.03125, m=8000000, k=5\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new filter based on parameters provided in example2\n",
    "n2 = 10**6\n",
    "m2 = 8 * 10**6\n",
    "f2 = 0\n",
    "bf2 = BloomFilter(n=n2, f=f2, m=m2)\n",
    "bf2.f = calculateF(bf2.k)\n",
    "bf2.printParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bb1a12-fcb6-4851-8f08-680dcf03476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init parameters:\n",
      "n = 100000000, f=0.03125, m=721347520, k=4\n"
     ]
    }
   ],
   "source": [
    "bf3 = BloomFilter(n = 100*bf2.n, f = bf2.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f1b0cd-b1ef-4245-bf53-91eb6fa4d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init parameters:\n",
      "n = 10000.0, f=0.03125, m=72134, k=4\n"
     ]
    }
   ],
   "source": [
    "bf4 = BloomFilter(n = 0.01*bf2.n, f = bf2.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cedc8611-e80c-4848-b4ec-bd67481d5eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of Bloom Filter where the dataset is 100 times larger than the original one: \n",
      "Init parameters:\n",
      "n = 100000000, f=0.03125, m=721347520, k=4\n",
      "Parameters of Bloom Filter where the dataset is 100 times smaller than the original one: \n",
      "Init parameters:\n",
      "n = 10000.0, f=0.03125, m=72134, k=4\n",
      "Ratio of the sizes between two new filters:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters of Bloom Filter where the dataset is 100 times larger than the original one: \")\n",
    "bf3.printParameters()\n",
    "print(\"Parameters of Bloom Filter where the dataset is 100 times smaller than the original one: \")\n",
    "bf4.printParameters()\n",
    "print(\"Ratio of the sizes between two new filters: \", round(bf3.m/bf4.m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96a097-56df-44fb-a138-1414aec86fcb",
   "metadata": {},
   "source": [
    "***Findings: What we found from increasing the size of dataset of Bloom Filter is that: as we increase the dataset size by 100 time, the number of bits in the Bloom Filter, or the size of filter, increases 100 times as well. As we shrink the dataset size by 100 times, the number of bits in the Bloom Filter/size of filter also decreases.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837780b8",
   "metadata": {},
   "source": [
    "## EXERCISE 3 from [page 59 of Algorithms and Data Structures for Massive Datasets](https://ebookcentral-proquest-com.proxy.library.georgetown.edu/lib/georgetown/reader.action?docID=7049417&ppg=64)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba340c",
   "metadata": {},
   "source": [
    "In some literature, a variant of a Bloom filter is described where different hash functions have the “jurisdiction” over different parts of the Bloom filter. In other words,  k  hash functions split the Bloom filter into  k  equal-sized consecutive chunks of  m/k  bits, and during an insert, the  i th hash function sets bits in the  i th chunk. Implement this variant of the Bloom filter and check if and how this change might affect the false positive rate in comparison to the original Bloom filter.\n",
    "\n",
    "Key requirements from Example 3:\n",
    "- n = len(lines) # the length of the tsv file\n",
    "- p = 0.2\n",
    "- for chunked bloom filter, it has 5 parts.\n",
    "\n",
    "Tasks: \n",
    "- Designed a chunked Bloom filter.\n",
    "- Compare the false positive rate of the chunked Bloom filter with that of the original one.\n",
    "- Make the conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab5e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import hashlib\n",
    "from bitarray import bitarray\n",
    "import re\n",
    "import math\n",
    "from a2.bloomfilter3 import StandardBloomFilter, ChunkedBloomFilter, ImprovedBloomFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c937a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    return text\n",
    "\n",
    "def tokenize_text(line):\n",
    "    parts = line.split('\\t')\n",
    "    tokens = []\n",
    "    for part in parts:\n",
    "        tokens.extend(part.split())\n",
    "    return tokens\n",
    "\n",
    "def calculate_bloom_filter_size(n, p):\n",
    "    m = - (n * math.log(p)) / (math.log(2) ** 2)\n",
    "    k = (m / n) * math.log(2)\n",
    "    return int(m), int(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ab2741-80c1-4972-968c-10658023fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: duplicates/threehundred.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0042\n",
      "Chunked Bloom Filter False Positive Rate: 0.0583\n",
      "File: duplicates/onek.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0138\n",
      "Chunked Bloom Filter False Positive Rate: 0.0700\n",
      "File: duplicates/tenk.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0145\n",
      "Chunked Bloom Filter False Positive Rate: 0.0648\n",
      "File: duplicates/hundredk.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0122\n",
      "Chunked Bloom Filter False Positive Rate: 0.0661\n"
     ]
    }
   ],
   "source": [
    "zip_path = '../duplicates.zip'\n",
    "required_files = ['duplicates/threehundred.tsv', 'duplicates/onek.tsv', 'duplicates/tenk.tsv', 'duplicates/hundredk.tsv']\n",
    "password = b'123456'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_files = zip_ref.namelist()\n",
    "\n",
    "    for file in required_files:\n",
    "        if file in zip_files:\n",
    "            print(f\"File: {file}\")\n",
    "            with zip_ref.open(file, pwd=password) as f:\n",
    "                lines = [line.decode('utf-8').strip() for line in f.readlines()]\n",
    "\n",
    "                n = len(lines)\n",
    "                p = 0.2\n",
    "                m, k = calculate_bloom_filter_size(n, p)\n",
    "\n",
    "                standard_bloom = StandardBloomFilter(m, k)\n",
    "                chunked_bloom = ChunkedBloomFilter(m, k, 5)\n",
    "\n",
    "                false_positive_standard = 0\n",
    "                false_positive_chunked = 0\n",
    "\n",
    "                elements_added = int(len(lines) * 0.20)\n",
    "                added_elements = lines[:elements_added]\n",
    "                test_elements = lines[elements_added:]\n",
    "\n",
    "                for line in added_elements:\n",
    "                    standard_bloom.add(line)\n",
    "                    chunked_bloom.add(line)\n",
    "\n",
    "                for line in test_elements:\n",
    "                    if standard_bloom.check(line):\n",
    "                        false_positive_standard += 1\n",
    "                    if chunked_bloom.check(line):\n",
    "                        false_positive_chunked += 1\n",
    "\n",
    "                if len(test_elements) > 0:\n",
    "                    print(f\"Standard Bloom Filter False Positive Rate: {false_positive_standard / len(test_elements):.4f}\")\n",
    "                    print(f\"Chunked Bloom Filter False Positive Rate: {false_positive_chunked / len(test_elements):.4f}\")\n",
    "        else:\n",
    "            print(f\"File {file} not found in ZIP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d9de5",
   "metadata": {},
   "source": [
    "***Findings: For all four TSV files, we can see that the false positive rate of the chunked Bloom filter is always higher than that of the standard Bloom filter. This indicates that with the part number set to 5, the performance of the original Bloom filter is better on these TSV files. The results might change if the part number is altered.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659a3de",
   "metadata": {},
   "source": [
    "# EXERCISE 3 - Addition from Assignment2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a4bee",
   "metadata": {},
   "source": [
    "Repeat exercise 3 by making two changes to your implementation of the Bloom filter in bloom_filter.py. How does the false positive rate change now? Specifically, explore three improvements of your choice to the Bloom filter. This is intentionally open-ended, and I’m excited to see what you experiment with. It is perfectly fine if you explore choices that do not lead to any substantial benefits. Improvements to Bloom filters are discussed in Algorithms and Data Structures for Massive Datasets, or you can find other materials online and cite them in your discussion. An example: the Kirsch-Mitzenmacher-Optimization computes two hashes instead of k hash functions. Here’s one more example: Hugging Face’s DataTrove has deduplication code that uses universal hashing to come up with k independent hash functions. We chose hash functions differently. Maybe you can try their approach. Or using a quotient filter is another option, as discussed in Algorithms and Data Structures for Massive Datasets. There are lots of options available to you. The most important point: think of your team as running experiments, exploring alternatives, and presenting your results to a stakeholder in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd42ba2",
   "metadata": {},
   "source": [
    "Key requirements from Example 3 - Addition:\n",
    "- n = len(lines) # the length of the tsv file\n",
    "- p = 0.2\n",
    "- for chunked bloom filter, it has 5 parts.\n",
    "- for improved bloom filter, the method here is kirsch-mitzenmacher\n",
    "\n",
    "Tasks: \n",
    "- Designed a improved Bloom filter with kirsch-mitzenmacher method.\n",
    "- Compare the false positive rate of the improved Bloom filter with that of the chunked and original one.\n",
    "- Make the conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34798981-8b06-49b6-90c0-5a865574f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: duplicates/threehundred.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0042\n",
      "Chunked Bloom Filter False Positive Rate: 0.0583\n",
      "Improved Bloom Filter False Positive Rate: 0.0125\n",
      "File: duplicates/onek.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0138\n",
      "Chunked Bloom Filter False Positive Rate: 0.0700\n",
      "Improved Bloom Filter False Positive Rate: 0.0150\n",
      "File: duplicates/tenk.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0145\n",
      "Chunked Bloom Filter False Positive Rate: 0.0648\n",
      "Improved Bloom Filter False Positive Rate: 0.0111\n",
      "File: duplicates/hundredk.tsv\n",
      "Standard Bloom Filter False Positive Rate: 0.0122\n",
      "Chunked Bloom Filter False Positive Rate: 0.0661\n",
      "Improved Bloom Filter False Positive Rate: 0.0129\n"
     ]
    }
   ],
   "source": [
    "zip_path = '../duplicates.zip'\n",
    "required_files = ['duplicates/threehundred.tsv', 'duplicates/onek.tsv', 'duplicates/tenk.tsv', 'duplicates/hundredk.tsv']\n",
    "password = b'123456'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_files = zip_ref.namelist()\n",
    "\n",
    "    for file in required_files:\n",
    "        if file in zip_files:\n",
    "            print(f\"File: {file}\")\n",
    "            with zip_ref.open(file, pwd=password) as f:\n",
    "                lines = [line.decode('utf-8').strip() for line in f.readlines()]\n",
    "\n",
    "                n = len(lines)\n",
    "                p = 0.2\n",
    "                m, k = calculate_bloom_filter_size(n, p)\n",
    "\n",
    "                standard_bloom = StandardBloomFilter(m, k)\n",
    "                chunked_bloom = ChunkedBloomFilter(m, k, 5)\n",
    "                improved_bloom = ImprovedBloomFilter(m, k, method='kirsch-mitzenmacher')\n",
    "\n",
    "                false_positive_standard = 0\n",
    "                false_positive_chunked = 0\n",
    "                false_positive_improved = 0\n",
    "\n",
    "                elements_added = int(len(lines) * 0.20)\n",
    "                added_elements = lines[:elements_added]\n",
    "                test_elements = lines[elements_added:]\n",
    "\n",
    "                for line in added_elements:\n",
    "                    standard_bloom.add(line)\n",
    "                    chunked_bloom.add(line)\n",
    "                    improved_bloom.add(line)\n",
    "\n",
    "                for line in test_elements:\n",
    "                    if standard_bloom.check(line):\n",
    "                        false_positive_standard += 1\n",
    "                    if chunked_bloom.check(line):\n",
    "                        false_positive_chunked += 1\n",
    "                    if improved_bloom.check(line):\n",
    "                        false_positive_improved += 1\n",
    "\n",
    "                if len(test_elements) > 0:\n",
    "                    print(f\"Standard Bloom Filter False Positive Rate: {false_positive_standard / len(test_elements):.4f}\")\n",
    "                    print(f\"Chunked Bloom Filter False Positive Rate: {false_positive_chunked / len(test_elements):.4f}\")\n",
    "                    print(f\"Improved Bloom Filter False Positive Rate: {false_positive_improved / len(test_elements):.4f}\")\n",
    "        else:\n",
    "            print(f\"File {file} not found in ZIP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187c215",
   "metadata": {},
   "source": [
    "***Findings: Comparing the results of four TSV files, we find that while the chunked Bloom filter always underperforms compared to the standard and improved Bloom filters, the performance between the standard and improved versions is uncertain. For the 300 and 1000 TSV files, the false positive rate of the improved Bloom filter is higher, but the situation changes with the 10k TSV file. Also, with the 100k TSV file, the performance of the two is almost identical. As parameters change, the results may also vary.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
